{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLyGGNWqob-I"
      },
      "source": [
        "*mounting to google drive*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14DYKVj3CBiT",
        "outputId": "2a1be869-dcd7-41ae-99d4-4d7bf706a081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkl_cXdPogPo"
      },
      "source": [
        "*install library prerequisites*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrL3tlE5ECcu",
        "outputId": "9cec2c54-2e70-4d2c-b56f-1a52e0afda8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scikit-learn pillow tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgvhmT_WQ-8j"
      },
      "source": [
        "*imports*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BkWZmEgnO2TW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, TimeDistributed, Flatten, Dropout, Input, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.layers import Reshape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zydXBpseyKVs"
      },
      "source": [
        "*script for finding the folders in drive*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCD4vP7dZ_jK",
        "outputId": "7e9984ca-de6b-4d8c-d730-0a20305791ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Contents of /content/drive/MyDrive/ASL_to_Text_Project:\n",
            "['data', 'models']\n",
            "\n",
            "Contents of /content/drive/MyDrive/ASL_to_Text_Project/data:\n",
            "['labels', 'images', 'gesture_sequences']\n"
          ]
        }
      ],
      "source": [
        "# Print the contents of the ASL_to_Text_Project directory\n",
        "project_dir = '/content/drive/MyDrive/ASL_to_Text_Project'\n",
        "print(f\"\\nContents of {project_dir}:\")\n",
        "print(os.listdir(project_dir))\n",
        "\n",
        "# Print the contents of the data directory\n",
        "data_dir = os.path.join(project_dir, 'data')\n",
        "print(f\"\\nContents of {data_dir}:\")\n",
        "print(os.listdir(data_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RcGgSfJLDjA"
      },
      "source": [
        "*initialize/configure*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b12pFhhGJI8k"
      },
      "outputs": [],
      "source": [
        "#  Configuration\n",
        "IMG_SIZE = 224  # Images will be resized to this size\n",
        "SEQUENCE_LENGTH = 30\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 50\n",
        "DATA_DIR = r\"/content/drive/My Drive/ASL_to_Text_Project/data\"\n",
        "IMAGES_DIR = os.path.join(DATA_DIR, 'images')\n",
        "SEQUENCES_DIR = os.path.join(DATA_DIR, 'gesture_sequences')\n",
        "MODEL_DIR = r\"/content/drive/My Drive/ASL_to_Text_Project/models\"\n",
        "LABELS_DIR = r\"/content/drive/My Drive/ASL_to_Text_Project/data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0aja5eyLH2u"
      },
      "source": [
        "*directory tweaks*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-WdeEKk5JQNr"
      },
      "outputs": [],
      "source": [
        "# Create directories if they don't exist\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(LABELS_DIR, exist_ok=True)\n",
        "os.makedirs(SEQUENCES_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TMpQIyLz7pK"
      },
      "source": [
        "*limit gpu memory growth*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HNsoUzVdz6TZ"
      },
      "outputs": [],
      "source": [
        "# # Limit GPU memory growth\n",
        "# gpus = tf.config.list_physical_devices('GPU')\n",
        "# if gpus:\n",
        "#     try:\n",
        "#         for gpu in gpus:\n",
        "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
        "#     except RuntimeError as e:\n",
        "#         print(e)\n",
        "\n",
        "# # Force TensorFlow to use the GPU\n",
        "# tf.config.set_visible_devices(gpus[0], 'GPU')\n",
        "# tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "\n",
        "# # Verify TensorFlow is using GPU\n",
        "# print(\"TensorFlow is using GPU:\", tf.test.is_built_with_cuda())\n",
        "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "# print(\"Devices:\", tf.config.list_physical_devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV524zHkE9I5"
      },
      "source": [
        "# Data Loading & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uEPdCKHME7Mi"
      },
      "outputs": [],
      "source": [
        "# --- Data Loading & Preprocessing ---\n",
        "\n",
        "#Collect labels for CNN data (from IMAGES_DIR)\n",
        "cnn_labels = os.listdir(IMAGES_DIR)\n",
        "#Collect labels for gesture sequences (from SEQUENCES_DIR)\n",
        "sequence_labels = os.listdir(SEQUENCES_DIR)\n",
        "#Fit separate LabelEncoders\n",
        "le_cnn = LabelEncoder()\n",
        "le_cnn.fit(cnn_labels)\n",
        "num_classes_cnn = len(le_cnn.classes_)\n",
        "\n",
        "le_sequence = LabelEncoder()\n",
        "le_sequence.fit(sequence_labels)\n",
        "num_classes_sequence = len(le_sequence.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load and preprocess iamge"
      ],
      "metadata": {
        "id": "8PvsDb9ZaUBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    return img_array"
      ],
      "metadata": {
        "id": "wM_p6zu1aUWM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzmnJUTPbJtT"
      },
      "source": [
        "# load cnn image function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JwLMcmqObJUX"
      },
      "outputs": [],
      "source": [
        "def load_cnn_data(data_dir, batch_size, gestures=None):\n",
        "    image_names = gestures if gestures is not None else os.listdir(data_dir)\n",
        "    while True:\n",
        "        np.random.shuffle(image_names)\n",
        "        for image_name in image_names:\n",
        "            image_dir = os.path.join(data_dir, image_name)\n",
        "            image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
        "            np.random.shuffle(image_files)\n",
        "            for i in range(0, len(image_files), batch_size):\n",
        "                batch_files = image_files[i:i+batch_size]\n",
        "                batch_images = []\n",
        "                batch_labels = []\n",
        "                for file_name in batch_files:\n",
        "                    image_path = os.path.join(image_dir, file_name)\n",
        "                    img_array = load_and_preprocess_image(image_path)\n",
        "                    batch_images.append(img_array)\n",
        "                    batch_labels.append(image_name)\n",
        "                yield np.array(batch_images), to_categorical(le_cnn.transform(batch_labels), num_classes=num_classes_cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9KIkLsXJYyv"
      },
      "source": [
        "# Load gesture sequences function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6IYfYcJVJcFM"
      },
      "outputs": [],
      "source": [
        "def load_gesture_data(data_dir, sequence_length, batch_size, gestures=None):\n",
        "    gesture_names = gestures if gestures is not None else os.listdir(data_dir)\n",
        "    batch_images = []\n",
        "    batch_labels = []\n",
        "\n",
        "    while True:\n",
        "        np.random.shuffle(gesture_names)\n",
        "        for gesture_name in gesture_names:\n",
        "            gesture_dir = os.path.join(data_dir, gesture_name)\n",
        "            sequence_folders = os.listdir(gesture_dir)\n",
        "            np.random.shuffle(sequence_folders)\n",
        "\n",
        "            for sequence_folder in sequence_folders:\n",
        "                sequence_path = os.path.join(gesture_dir, sequence_folder)\n",
        "                image_files = [\n",
        "                    f for f in os.listdir(sequence_path) if f.endswith('.jpg')\n",
        "                ]\n",
        "                image_files.sort()\n",
        "\n",
        "                if len(image_files) < sequence_length:\n",
        "                    continue\n",
        "\n",
        "                start_idx = np.random.randint(\n",
        "                    0, len(image_files) - sequence_length + 1\n",
        "                )\n",
        "                frames = []\n",
        "                for i in range(start_idx, start_idx + sequence_length):\n",
        "                    frame_path = os.path.join(sequence_path, image_files[i])\n",
        "                    img_array = load_and_preprocess_image(frame_path)\n",
        "                    frames.append(img_array)\n",
        "\n",
        "                # Encode the gesture_name before one-hot encoding\n",
        "                numerical_label = le_sequence.transform([gesture_name])[0]\n",
        "                batch_images.append(np.array(frames))\n",
        "                batch_labels.append(numerical_label)\n",
        "\n",
        "                # Yield a batch when enough samples are accumulated\n",
        "                if len(batch_images) == batch_size:\n",
        "                    yield np.array(batch_images), to_categorical(\n",
        "                        batch_labels, num_classes=num_classes_sequence\n",
        "                    )\n",
        "                    batch_images = []  # Reset for the next batch\n",
        "                    batch_labels = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmP4t0xiTk7D"
      },
      "source": [
        "# Training with RNN + CNN training model\n",
        "*hybrid of rnn and cnn: Convolutional Neural Network - Long Short-Term Memory (CNN-LSTM)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Mr2afLWkJXlt"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Reshape\n",
        "\n",
        "def create_cnn_lstm_model(num_classes_cnn, num_classes_sequence):\n",
        "    # --- CNN Branch ---\n",
        "    cnn_input = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"cnn_input\")\n",
        "    x = Conv2D(64, kernel_size=(3, 3), activation='relu')(cnn_input)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Conv2D(128, kernel_size=(3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Conv2D(128, kernel_size=(3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    cnn_output = Flatten()(x)\n",
        "    print(\"CNN Output Shape:\", cnn_output.shape)  # Print shape for debugging\n",
        "\n",
        "    # --- RNN Branch ---\n",
        "    rnn_input = Input(shape=(SEQUENCE_LENGTH, IMG_SIZE, IMG_SIZE, 3), name=\"rnn_input\")\n",
        "    y = TimeDistributed(Conv2D(64, kernel_size=(3, 3), activation='relu'))(rnn_input)\n",
        "    y = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(y)\n",
        "    y = TimeDistributed(Conv2D(128, kernel_size=(3, 3), activation='relu'))(y)\n",
        "    y = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(y)\n",
        "    y = TimeDistributed(Conv2D(128, kernel_size=(3, 3), activation='relu'))(y)\n",
        "    y = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(y)\n",
        "    y = TimeDistributed(Flatten())(y)\n",
        "    print(\"Shape after TimeDistributed Flatten:\", y.shape)\n",
        "\n",
        "    y = LSTM(256, return_sequences=False)(y)  # return_sequences=False for feature extraction\n",
        "    print(\"Shape after LSTM:\", y.shape)\n",
        "\n",
        "    rnn_output = y\n",
        "    print(\"Shape after LSTM (final):\", rnn_output.shape)\n",
        "\n",
        "    # --- Reshape RNN Output ---\n",
        "    rnn_output = Reshape((BATCH_SIZE, 1, -1))(rnn_output)\n",
        "    print(\"Shape after Reshape:\", rnn_output.shape)\n",
        "\n",
        "    # --- Flatten the RNN output ---\n",
        "    rnn_output = Flatten()(rnn_output)\n",
        "    print(\"Shape after Flatten:\", rnn_output.shape)\n",
        "\n",
        "    # --- Concatenate CNN and Flattened RNN Outputs ---\n",
        "    merged_output = concatenate([cnn_output, rnn_output])\n",
        "    print(\"Shape after Concatenate:\", merged_output.shape)\n",
        "\n",
        "    # --- Dense Layers for Classification ---\n",
        "    cnn_final_output = Dense(num_classes_cnn, activation='softmax', name='cnn_output')(merged_output)\n",
        "    sequence_final_output = Dense(num_classes_sequence, activation='softmax', name='sequence_output')(merged_output)\n",
        "\n",
        "    # --- Model Definition ---\n",
        "    model = Model(inputs=[cnn_input, rnn_input], outputs=[cnn_final_output, sequence_final_output])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxAY3Ug4FRxs"
      },
      "source": [
        "# Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "05Oi_OD6FOXS"
      },
      "outputs": [],
      "source": [
        "# Data Splitting\n",
        "cnn_gestures_train = cnn_labels[:int(len(cnn_labels) * 0.8)]\n",
        "cnn_gestures_val = cnn_labels[int(len(cnn_labels) * 0.8):]\n",
        "sequence_gestures_train = sequence_labels[:int(len(sequence_labels) * 0.8)]\n",
        "sequence_gestures_val = sequence_labels[int(len(sequence_labels) * 0.8):]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXdFRgTu_uCY"
      },
      "source": [
        "*Mixed Precision*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6Ve_HFlT_tiC"
      },
      "outputs": [],
      "source": [
        "# Mixed Precision\n",
        "from tensorflow.keras import mixed_precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LQu0RZNrSyF"
      },
      "source": [
        "# Calculate Steps per Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YRr7nTwUrQdn"
      },
      "outputs": [],
      "source": [
        "# Calculate Steps per Epoch\n",
        "total_training_samples = sum(len(os.listdir(os.path.join(SEQUENCES_DIR, gesture)))\n",
        "                            for gesture in sequence_gestures_train)\n",
        "total_validation_samples = sum(len(os.listdir(os.path.join(SEQUENCES_DIR, gesture)))\n",
        "                             for gesture in sequence_gestures_val)\n",
        "\n",
        "steps_per_epoch = total_training_samples // BATCH_SIZE\n",
        "validation_steps = total_validation_samples // BATCH_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataset objects"
      ],
      "metadata": {
        "id": "Czijjnf8egyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create tf.data.Dataset objects ---\n",
        "train_cnn_data = tf.data.Dataset.from_generator(\n",
        "    lambda: load_cnn_data(IMAGES_DIR, BATCH_SIZE, gestures=cnn_gestures_train),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, num_classes_cnn), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "\n",
        "train_sequence_data = tf.data.Dataset.from_generator(\n",
        "    lambda: load_gesture_data(SEQUENCES_DIR, SEQUENCE_LENGTH, BATCH_SIZE, gestures=sequence_gestures_train),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, SEQUENCE_LENGTH, IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, num_classes_sequence), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "\n",
        "train_dataset = tf.data.Dataset.zip((train_cnn_data, train_sequence_data))\n",
        "train_dataset = train_dataset.map(lambda cnn, seq: ({'cnn_input': cnn[0], 'rnn_input': seq[0]},\n",
        "                                                    {'cnn_output': cnn[1], 'sequence_output': seq[1]}))\n",
        "\n"
      ],
      "metadata": {
        "id": "rLvO5-0mefP_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# validation dataset"
      ],
      "metadata": {
        "id": "Zm7OOK41erG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Validation Dataset (Follow the same pattern as training data) ---\n",
        "test_cnn_data = tf.data.Dataset.from_generator(\n",
        "    lambda: load_cnn_data(IMAGES_DIR, BATCH_SIZE, gestures=cnn_gestures_test),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, num_classes_cnn), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "\n",
        "test_sequence_data = tf.data.Dataset.from_generator(\n",
        "    lambda: load_gesture_data(SEQUENCES_DIR, SEQUENCE_LENGTH, BATCH_SIZE, gestures=sequence_gestures_test),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, SEQUENCE_LENGTH, IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, num_classes_sequence), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "\n",
        "test_dataset = tf.data.Dataset.zip((test_cnn_data, test_sequence_data))\n",
        "test_dataset = test_dataset.map(lambda cnn, seq: ({'cnn_input': cnn[0], 'rnn_input': seq[0]},\n",
        "                                                    {'cnn_output': cnn[1], 'sequence_output': seq[1]}))"
      ],
      "metadata": {
        "id": "-1MNDiAweraA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv3AZHc-Jz6W"
      },
      "source": [
        "# Model compilation and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE9izAY0JzjI",
        "outputId": "1d3b078e-f716-43b6-902d-b3913ed5bca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN Output Shape: (None, 86528)\n",
            "Shape after TimeDistributed Flatten: (None, 30, 86528)\n",
            "Shape after LSTM: (None, 256)\n",
            "Shape after LSTM (final): (None, 256)\n",
            "Shape after Reshape: (None, 16, 1, 16)\n",
            "Shape after Flatten: (None, 256)\n",
            "Shape after Concatenate: (None, 86784)\n",
            "Epoch 1/50\n"
          ]
        }
      ],
      "source": [
        "# --- Model Compilation & Training ---\n",
        "model = create_cnn_lstm_model(num_classes_cnn, num_classes_sequence)\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss={\n",
        "        'cnn_output': 'categorical_crossentropy',\n",
        "        'sequence_output': 'categorical_crossentropy'\n",
        "    },\n",
        "    metrics={\n",
        "        'cnn_output': ['accuracy'],\n",
        "        'sequence_output': ['accuracy']\n",
        "    }\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=test_dataset,\n",
        "    validation_steps=validation_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72yZE1bqKAbg"
      },
      "source": [
        "# Test Set Evaluation and saving\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y4FfgQdKECC"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "test_loss, cnn_test_loss, sequence_test_loss, cnn_test_acc, sequence_test_acc = model.evaluate(val_dataset, steps=validation_steps)\n",
        "print(f\"CNN Test accuracy: {cnn_test_acc:.4f}\")\n",
        "print(f\"Sequence Test accuracy: {sequence_test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a new dataset with encoded labels for evaluation"
      ],
      "metadata": {
        "id": "gPdt7rJ_gZ8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new dataset with encoded labels for evaluation\n",
        "def encode_labels(input_batch):\n",
        "    images, labels = input_batch\n",
        "    encoded_labels = to_categorical(le_sequence.transform(labels), num_classes=num_classes_sequence)\n",
        "    return images, encoded_labels\n",
        "\n",
        "# Use 'map' to apply the label encoding to the test dataset\n",
        "encoded_test_dataset = test_dataset.map(encode_labels)\n",
        "\n",
        "# Evaluate the model on the dataset with encoded labels\n",
        "test_loss, cnn_test_loss, sequence_test_loss, cnn_test_acc, sequence_test_acc = model.evaluate(\n",
        "    encoded_test_dataset,\n",
        "    steps=validation_steps\n",
        ")\n",
        "print(f\"CNN Test accuracy: {cnn_test_acc:.4f}\")\n",
        "print(f\"Sequence Test accuracy: {sequence_test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "_dOkxHKBgZD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob_KRGkWKGNS"
      },
      "source": [
        "# Saving the model and label encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVGE9iI2KHj7"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model_save_path = os.path.join(MODEL_DIR, \"model.h5\")\n",
        "model.save(model_save_path)\n",
        "print(f\"Model saved to: {model_save_path}\")\n",
        "\n",
        "# Save the label encoders\n",
        "le_cnn_path = os.path.join(LABELS_DIR, 'le_cnn.pkl')\n",
        "le_sequence_path = os.path.join(LABELS_DIR, 'le_seq.pkl')\n",
        "\n",
        "with open(le_cnn_path, 'wb') as f:\n",
        "    pickle.dump(le_cnn, f)\n",
        "with open(le_sequence_path, 'wb') as f:\n",
        "    pickle.dump(le_sequence, f)\n",
        "\n",
        "print(f\"Label encoders saved to: {le_cnn_path} and {le_sequence_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr943CUWKakZ"
      },
      "source": [
        "# Calculate total images trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujd1694AKgRc"
      },
      "outputs": [],
      "source": [
        "#Calculate and print total images trained on\n",
        "total_images_trained = total_training_samples * SEQUENCE_LENGTH\n",
        "print(f\"Total Images Trained On: {total_images_trained}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxPAWIg-Khw8"
      },
      "source": [
        "# Plot Training and Validation curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkEScYJCKq41"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['cnn_output_accuracy'], label='CNN Training Accuracy')\n",
        "plt.plot(history.history['val_cnn_output_accuracy'], label='CNN Validation Accuracy')\n",
        "plt.plot(history.history['sequence_output_accuracy'], label='Sequence Training Accuracy')\n",
        "plt.plot(history.history['val_sequence_output_accuracy'], label='Sequence Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['cnn_output_loss'], label='CNN Training Loss')\n",
        "plt.plot(history.history['val_cnn_output_loss'], label='CNN Validation Loss')\n",
        "plt.plot(history.history['sequence_output_loss'], label='Sequence Training Loss')\n",
        "plt.plot(history.history['val_sequence_output_loss'], label='Sequence Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}