{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLyGGNWqob-I"
      },
      "source": [
        "*mounting to google drive*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14DYKVj3CBiT",
        "outputId": "431cad70-1a25-4b81-aa58-52e7a3e0a04c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkl_cXdPogPo"
      },
      "source": [
        "*install library prerequisites*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrL3tlE5ECcu",
        "outputId": "ac513fe8-dbd0-4aa4-d34f-61a3339ca42e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scikit-learn pillow tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgvhmT_WQ-8j"
      },
      "source": [
        "*imports*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkWZmEgnO2TW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, TimeDistributed, Flatten, Dropout, Input, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zydXBpseyKVs"
      },
      "source": [
        "*script for finding the folders in drive*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCD4vP7dZ_jK"
      },
      "outputs": [],
      "source": [
        "# Print the contents of the ASL_to_Text_Project directory\n",
        "project_dir = '/content/drive/MyDrive/ASL_to_Text_Project'\n",
        "print(f\"\\nContents of {project_dir}:\")\n",
        "print(os.listdir(project_dir))\n",
        "\n",
        "# Print the contents of the data directory\n",
        "data_dir = os.path.join(project_dir, 'data')\n",
        "print(f\"\\nContents of {data_dir}:\")\n",
        "print(os.listdir(data_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RcGgSfJLDjA"
      },
      "source": [
        "*initialize/configure*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b12pFhhGJI8k"
      },
      "outputs": [],
      "source": [
        "#  Configuration\n",
        "IMG_SIZE = 224  # Images will be resized to this size\n",
        "SEQUENCE_LENGTH = 30\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 50\n",
        "DATA_DIR = r\"/content/drive/My Drive/ASL_to_Text_Project/data\"\n",
        "IMAGES_DIR = os.path.join(DATA_DIR, 'images')\n",
        "SEQUENCES_DIR = os.path.join(DATA_DIR, 'gesture_sequences')\n",
        "MODEL_DIR = r\"/content/drive/My Drive/ASL_to_Text_Project/models\"\n",
        "LABELS_DIR = r\"/content/drive/My Drive/ASL_to_Text_Project/data/\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(LABELS_DIR, exist_ok=True)\n",
        "os.makedirs(SEQUENCES_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0aja5eyLH2u"
      },
      "source": [
        "*directory tweaks*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WdeEKk5JQNr"
      },
      "outputs": [],
      "source": [
        "# Create directories if they don't exist\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(LABELS_DIR, exist_ok=True)\n",
        "os.makedirs(SEQUENCES_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*limit gpu memory growth*"
      ],
      "metadata": {
        "id": "0TMpQIyLz7pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "HNsoUzVdz6TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9KIkLsXJYyv"
      },
      "source": [
        "# Load gesture sequences function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IYfYcJVJcFM"
      },
      "outputs": [],
      "source": [
        "#  Data Loading cnn&rnn\n",
        "def load_gesture_data(data_dir, sequence_length, batch_size, gestures=None):\n",
        "    \"\"\"Loads gesture data and resizes images to IMG_SIZE.\"\"\"\n",
        "\n",
        "    gesture_names = gestures if gestures is not None else os.listdir(data_dir)\n",
        "    num_gestures = len(gesture_names)\n",
        "\n",
        "    while True:\n",
        "        for gesture_idx in range(num_gestures):\n",
        "            gesture_dir = os.path.join(data_dir, gesture_names[gesture_idx])\n",
        "            for sequence_folder in os.listdir(gesture_dir):\n",
        "                sequence_path = os.path.join(gesture_dir, sequence_folder)\n",
        "                image_files = [f for f in os.listdir(sequence_path) if f.endswith('.jpg')]\n",
        "                image_files.sort(key=lambda x: int(x.split('_')[2].split(' ')[0].split('.')[0]))\n",
        "                num_images = len(image_files)\n",
        "\n",
        "                num_batches = (num_images - sequence_length + 1) // sequence_length\n",
        "\n",
        "                for batch_idx in range(num_batches):\n",
        "                    batch_sequences = []\n",
        "                    batch_labels = []\n",
        "\n",
        "                    for i in range(batch_idx * sequence_length, (batch_idx + 1) * sequence_length):\n",
        "                        frames = []\n",
        "                        for j in range(i, i + sequence_length):\n",
        "                            frame_path = os.path.join(sequence_path, image_files[j])\n",
        "                            # Resize image during loading\n",
        "                            img = load_img(frame_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
        "                            img_array = img_to_array(img) / 255.0\n",
        "                            frames.append(img_array)\n",
        "                        batch_sequences.append(frames)\n",
        "                        batch_labels.append(gesture_names[gesture_idx])\n",
        "\n",
        "                    yield np.array(batch_sequences), to_categorical(le.transform(batch_labels), num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzmnJUTPbJtT"
      },
      "source": [
        "# load cnn image function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwLMcmqObJUX"
      },
      "outputs": [],
      "source": [
        "def load_cnn_data(data_dir, batch_size, gestures=None):\n",
        "    \"\"\"Loads CNN data and resizes images to IMG_SIZE.\"\"\"\n",
        "\n",
        "    image_names = gestures if gestures is not None else os.listdir(data_dir)\n",
        "    num_images = len(image_names)\n",
        "\n",
        "    while True:\n",
        "        for image_idx in range(num_images):\n",
        "            image_dir = os.path.join(data_dir, image_names[image_idx])\n",
        "            image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
        "            image_files.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
        "\n",
        "            num_batches = len(image_files) // batch_size\n",
        "\n",
        "            for batch_idx in range(num_batches):\n",
        "                batch_images = []\n",
        "                batch_labels = []\n",
        "                for i in range(batch_idx * batch_size, (batch_idx + 1) * batch_size):\n",
        "                    file_name = image_files[i]\n",
        "                    image_path = os.path.join(image_dir, file_name)\n",
        "                    # Resize image during loading\n",
        "                    img = load_img(image_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
        "                    img_array = img_to_array(img) / 255.0\n",
        "                    batch_images.append(img_array)\n",
        "                    batch_labels.append(image_names[image_idx])\n",
        "\n",
        "                yield np.array(batch_images), to_categorical(le.transform(batch_labels), num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "ifSgfUzsBkqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "def augment_data(images, labels, batch_size):\n",
        "    \"\"\"Applies data augmentation to the input images and labels.\"\"\"\n",
        "    for x_batch, y_batch in datagen.flow(images, labels, batch_size=batch_size):\n",
        "        yield x_batch, y_batch"
      ],
      "metadata": {
        "id": "leU6SaaQBkEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Data Handling*"
      ],
      "metadata": {
        "id": "npzIIECw_jpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(images, labels, batch_size, shuffle=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(images))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "nU5eYTRA_izf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Mixed Precision*"
      ],
      "metadata": {
        "id": "uXdFRgTu_uCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)"
      ],
      "metadata": {
        "id": "6Ve_HFlT_tiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xb7bAPMeQ3u"
      },
      "source": [
        "*load and split data for cnn and rnn*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_d_3LQxJkRh"
      },
      "outputs": [],
      "source": [
        "# Get a list of all gesture names\n",
        "all_gestures = os.listdir(SEQUENCES_DIR)\n",
        "# Calculate the split index\n",
        "split_index = int(len(all_gestures) * 0.8) # Assuming 80/20 split\n",
        "\n",
        "# Split gesture names into training and validation sets\n",
        "train_gestures = all_gestures[:split_index]\n",
        "val_gestures = all_gestures[split_index:]\n",
        "\n",
        "# Create separate training and validation data generators\n",
        "train_generator_rnn = load_gesture_data(SEQUENCES_DIR, SEQUENCE_LENGTH, BATCH_SIZE, gestures=train_gestures)\n",
        "train_generator_cnn = load_cnn_data(IMAGES_DIR, BATCH_SIZE, gestures=train_gestures)\n",
        "\n",
        "val_generator_rnn = load_gesture_data(SEQUENCES_DIR, SEQUENCE_LENGTH, BATCH_SIZE, gestures=val_gestures)\n",
        "val_generator_cnn = load_cnn_data(IMAGES_DIR, BATCH_SIZE, gestures=val_gestures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmP4t0xiTk7D"
      },
      "source": [
        "# Training with RNN + CNN training model\n",
        "*hybrid of rnn and cnn: Convolutional Neural Network - Long Short-Term Memory (CNN-LSTM)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr2afLWkJXlt"
      },
      "outputs": [],
      "source": [
        "def create_cnn_lstm_model(num_classes):\n",
        "    # --- CNN Branch ---\n",
        "    cnn_input = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"cnn_input\")\n",
        "    x = Conv2D(96, kernel_size=(11, 11), strides=(4, 4), activation='relu',\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(0.01))(cnn_input)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "    x = Conv2D(256, kernel_size=(5, 5), activation='relu', padding='same',\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "    x = Conv2D(384, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "    x = Conv2D(384, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "    x = Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
        "    cnn_output = Flatten()(x)\n",
        "\n",
        "    # --- RNN Branch ---\n",
        "    rnn_input = Input(shape=(SEQUENCE_LENGTH, IMG_SIZE, IMG_SIZE, 3), name=\"rnn_input\")\n",
        "    y = TimeDistributed(Conv2D(96, kernel_size=(11, 11), strides=(4, 4), activation='relu',\n",
        "                               kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "                               input_shape=(IMG_SIZE, IMG_SIZE, 3)))(rnn_input)\n",
        "    y = TimeDistributed(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))(y)\n",
        "    y = TimeDistributed(Conv2D(256, kernel_size=(5, 5), activation='relu', padding='same',\n",
        "                               kernel_regularizer=tf.keras.regularizers.l2(0.01)))(y)\n",
        "    y = TimeDistributed(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))(y)\n",
        "    y = TimeDistributed(Conv2D(384, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "                               kernel_regularizer=tf.keras.regularizers.l2(0.01)))(y)\n",
        "    y = TimeDistributed(Conv2D(384, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "                               kernel_regularizer=tf.keras.regularizers.l2(0.01)))(y)\n",
        "    y = TimeDistributed(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "                               kernel_regularizer=tf.keras.regularizers.l2(0.01)))(y)\n",
        "    y = TimeDistributed(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))(y)\n",
        "    y = TimeDistributed(Flatten())(y)\n",
        "    y = LSTM(256, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01))(y)\n",
        "    y = Dropout(0.5)(y)\n",
        "    rnn_output = LSTM(256, kernel_regularizer=tf.keras.regularizers.l2(0.01))(y)\n",
        "\n",
        "    # --- Combine Outputs ---\n",
        "    merged_output = concatenate([cnn_output, rnn_output])\n",
        "    final_output = Dense(num_classes, activation='softmax')(merged_output)\n",
        "\n",
        "    # --- Create the Model ---\n",
        "    model = Model(inputs=[cnn_input, rnn_input], outputs=final_output)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load and split data"
      ],
      "metadata": {
        "id": "dH228G8y5LPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load and split data\n",
        "all_gestures = os.listdir(SEQUENCES_DIR)\n",
        "split_index = int(len(all_gestures) * 0.8)\n",
        "\n",
        "train_gestures = all_gestures[:split_index]\n",
        "val_gestures = all_gestures[split_index:]"
      ],
      "metadata": {
        "id": "orRYgNjn5GyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NmQR9ZsJm3-"
      },
      "source": [
        "# Encode Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFhAFQGKJl1z"
      },
      "outputs": [],
      "source": [
        "# Encode Labels\n",
        "# Collect all labels from your training data\n",
        "all_train_labels = []\n",
        "for gesture in train_gestures:\n",
        "    gesture_dir = os.path.join(SEQUENCES_DIR, gesture)\n",
        "    for sequence_folder in os.listdir(gesture_dir):\n",
        "        all_train_labels.append(gesture)\n",
        "# Fit the LabelEncoder on all training labels\n",
        "le = LabelEncoder()\n",
        "le.fit(all_train_labels)\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "train_generator_rnn = load_gesture_data(SEQUENCES_DIR, SEQUENCE_LENGTH, BATCH_SIZE, gestures=train_gestures)\n",
        "train_generator_cnn = load_cnn_data(IMAGES_DIR, BATCH_SIZE, gestures=train_gestures)\n",
        "\n",
        "val_generator_rnn = load_gesture_data(SEQUENCES_DIR, SEQUENCE_LENGTH, BATCH_SIZE, gestures=val_gestures)\n",
        "val_generator_cnn = load_cnn_data(IMAGES_DIR, BATCH_SIZE, gestures=val_gestures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv3AZHc-Jz6W"
      },
      "source": [
        "# Model compilation and creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE9izAY0JzjI"
      },
      "outputs": [],
      "source": [
        "# Create and compile model\n",
        "model = create_cnn_lstm_model(num_classes)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Steps per Epoch"
      ],
      "metadata": {
        "id": "-LQu0RZNrSyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Steps per Epoch\n",
        "total_training_samples = sum(len(os.listdir(os.path.join(SEQUENCES_DIR, gesture)))\n",
        "                            for gesture in train_gestures) # Count samples only in training gestures\n",
        "total_validation_samples = sum(len(os.listdir(os.path.join(SEQUENCES_DIR, gesture)))\n",
        "                             for gesture in val_gestures)  # Count samples only in validation gestures\n",
        "\n",
        "steps_per_epoch = total_training_samples // BATCH_SIZE\n",
        "validation_steps = total_validation_samples // BATCH_SIZE"
      ],
      "metadata": {
        "id": "YRr7nTwUrQdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*mixed precision*"
      ],
      "metadata": {
        "id": "4KqTzBVe5qP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Mixed Precision\n",
        "from tensorflow.keras import mixed_precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)"
      ],
      "metadata": {
        "id": "yP7xAYfH5nP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJFkQtRIJ6yf"
      },
      "source": [
        "# Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Qh-G-96J8j_"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    x={\"cnn_input\": train_generator_cnn, \"rnn_input\": train_generator_rnn},\n",
        "    y=train_generator_rnn,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=({\"cnn_input\": val_generator_cnn, \"rnn_input\": val_generator_rnn}, val_generator_rnn),\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=os.path.join(MODEL_DIR, \"gesture_model_{epoch:02d}_{val_accuracy:.2f}.h5\"),\n",
        "            monitor=\"val_accuracy\",\n",
        "            save_best_only=True,\n",
        "            mode=\"max\",\n",
        "        ),\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72yZE1bqKAbg"
      },
      "source": [
        "# Test Set Evaluation and saving\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y4FfgQdKECC"
      },
      "outputs": [],
      "source": [
        "#Test Set Evaluation & Saving\n",
        "def evaluate_generator(generator, steps):\n",
        "    \"\"\"Evaluates the model on a data generator.\"\"\"\n",
        "    all_preds = []\n",
        "    all_true = []\n",
        "    for _ in tqdm(range(steps), desc=\"Evaluating\"):\n",
        "        X_batch_rnn, y_batch = next(generator)\n",
        "        X_batch_cnn, _ = next(generator)\n",
        "\n",
        "        # Predict and convert to class labels\n",
        "        y_pred = model.predict({\"cnn_input\": X_batch_cnn, \"rnn_input\": X_batch_rnn})\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        y_true_classes = np.argmax(y_batch, axis=1)\n",
        "\n",
        "        all_preds.extend(y_pred_classes)\n",
        "        all_true.extend(y_true_classes)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = np.mean(np.equal(all_true, all_preds))\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "evaluate_generator(val_generator_rnn, validation_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob_KRGkWKGNS"
      },
      "source": [
        "# Saving the model and label encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVGE9iI2KHj7"
      },
      "outputs": [],
      "source": [
        "#Save Model and Label Encoder\n",
        "model.save(os.path.join(MODEL_DIR, \"gesture_model.h5\"))\n",
        "with open(os.path.join(LABELS_DIR, 'gesture_label_encoder.pkl'), 'wb') as f:\n",
        "    pickle.dump(le, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr943CUWKakZ"
      },
      "source": [
        "# Calculate total images trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujd1694AKgRc"
      },
      "outputs": [],
      "source": [
        "#Calculate and print total images trained on\n",
        "total_images_trained = total_training_samples * SEQUENCE_LENGTH\n",
        "print(f\"Total Images Trained On: {total_images_trained}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxPAWIg-Khw8"
      },
      "source": [
        "# Plot Training and Validation curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkEScYJCKq41"
      },
      "outputs": [],
      "source": [
        "#Plot Training Curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "h9KIkLsXJYyv",
        "GzmnJUTPbJtT",
        "ifSgfUzsBkqm",
        "YmP4t0xiTk7D",
        "dH228G8y5LPL",
        "8NmQR9ZsJm3-",
        "Lv3AZHc-Jz6W",
        "-LQu0RZNrSyF",
        "lJFkQtRIJ6yf"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}